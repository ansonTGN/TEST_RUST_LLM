# ğŸ¦€ multi_llm_client Â· Cliente Rust para LLMs vÃ­a HTTP

[![License: MIT](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![Rust](https://img.shields.io/badge/Rust-1.70%2B-orange.svg)](https://www.rust-lang.org/)
[![Axum](https://img.shields.io/badge/Framework-Axum-4B275F.svg)](https://docs.rs/axum/latest/axum/)

---

## ğŸš€ DescripciÃ³n

Este proyecto es un **servidor web en Rust** construido con [Axum](https://github.com/tokio-rs/axum) que permite enviar *prompts* a modelos de lenguaje (LLMs) alojados en servicios como [Ollama](https://ollama.com/), y recibir sus respuestas de forma estructurada.

AdemÃ¡s, proporciona endpoints de estado y sirve un frontend web bÃ¡sico desde `/public`.

---

## ğŸ“š Endpoints disponibles

| MÃ©todo | Ruta             | DescripciÃ³n                                               |
|--------|------------------|-----------------------------------------------------------|
| `GET`  | `/`              | Sirve el archivo `index.html` desde el directorio `public`. |
| `GET`  | `/health`        | Comprueba si el servidor estÃ¡ activo (200 OK).           |
| `GET`  | `/status`        | Devuelve un JSON con el estado del servidor.             |
| `POST` | `/api/prompt`    | EnvÃ­a un prompt a un LLM remoto vÃ­a HTTP.                |
| `GET`  | `/api/models`    | Obtiene los modelos disponibles desde un servidor Ollama.|

---

## ğŸ› ï¸ InstalaciÃ³n y ejecuciÃ³n

1. **Clonar el repositorio:**

```bash
git clone https://github.com/tu-usuario/multi_llm_client.git
cd multi_llm_client
````

2. **Instalar dependencias (si es necesario):**

```bash
cargo build
```

3. **Ejecutar el servidor:**

```bash
cargo run
```

ğŸ“ El servidor quedarÃ¡ escuchando en:

```
http://127.0.0.1:8080
```

---

## ğŸ§ª Ejemplo de uso (API `/api/prompt`)

### Solicitud `POST`:

```json
{
  "url": "http://127.0.0.1:11434",
  "model": "gemma3:12b",
  "prompt": "Â¿CuÃ¡l es la capital de EspaÃ±a?"
}
```

### Respuesta esperada:

```json
{
  "response": "La capital de EspaÃ±a es **Madrid**."
}
```

---

## ğŸ§© Dependencias principales

* [`axum`](https://docs.rs/axum) â€“ Framework web asÃ­ncrono en Rust.
* [`reqwest`](https://docs.rs/reqwest) â€“ Cliente HTTP robusto.
* [`tokio`](https://tokio.rs) â€“ Runtime asÃ­ncrono.
* [`serde`](https://serde.rs) â€“ SerializaciÃ³n y deserializaciÃ³n de datos.
* [`futures-util`](https://docs.rs/futures-util) â€“ Manejo avanzado de streams.
* [`tokio-util`](https://docs.rs/tokio-util) â€“ Adaptadores Ãºtiles para Tokio.

---

## ğŸ“ Estructura del proyecto

```
multi_llm_client/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ main.rs           # CÃ³digo principal del servidor
â”œâ”€â”€ public/
â”‚   â””â”€â”€ index.html        # Interfaz web frontend
â”œâ”€â”€ Cargo.toml            # ConfiguraciÃ³n de dependencias
â””â”€â”€ README.md             # Este archivo
```

---

## âš–ï¸ Licencia

Este proyecto estÃ¡ licenciado bajo la licencia MIT. Ver el archivo [LICENSE](LICENSE) para mÃ¡s detalles.

---

## ğŸ‘¨â€ğŸ’» Autor

Desarrollado por **Ãngel A. Urbina**, 2025.

---

## ğŸ™‹â€â™€ï¸ Contribuciones

Â¡Contribuciones, *issues* y *pull requests* son bienvenidos!

---

## ğŸŒ Recursos relacionados

* [Ollama](https://ollama.com/) â€“ Servidor local para modelos de lenguaje
* [Axum](https://docs.rs/axum) â€“ Framework web asÃ­ncrono en Rust
* [OpenAI API](https://platform.openai.com/docs/api-reference)

